{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wholesale customers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'zipfile' has no attribute 'ZipFile'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as snb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import xgboost \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import cv\n",
    "from sklearn.preprocessing import  StandardScaler , MinMaxScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"default\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanatory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Wholesale customers data.xlsx\")\n",
    "print (\"data is read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Region.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Channel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df).describe().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling to Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing MinMax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_MinMax = MinMaxScaler().fit_transform(df)\n",
    "scaler_MinMax[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scaled data back to a DataFrame with the same columns\n",
    "scaled_df = pd.DataFrame(scaler_MinMax, columns=df.columns)\n",
    "\n",
    "# Now, scaled_df contains the scaled data in the same structure as df\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler_MinMax).describe().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_Standard = StandardScaler().fit_transform(df)\n",
    "scaler_Standard[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler_Standard).describe().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scaled data back to a DataFrame with the same columns\n",
    "scaled_Standard_df = pd.DataFrame(scaler_Standard, columns=df.columns)\n",
    "\n",
    "# Now, scaled_df contains the scaled data in the same structure as df\n",
    "print(scaled_Standard_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your features (X) and target (y)\n",
    "X = scaled_df.drop(columns=['Channel'])\n",
    "y = scaled_df['Channel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow method for KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of K values to test \n",
    "K_values = range(2, 16)\n",
    "\n",
    "# Initialize an empty list to store the inertia (within-cluster sum of squares) values\n",
    "inertia = []\n",
    "\n",
    "# Iterate over the K values and fit KMeans for each K\n",
    "for k in K_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    # Append the inertia value to the list\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow method to identify the optimal K\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K_values, inertia, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Identify the optimal K using the \"elbow\" method\n",
    "diff = np.diff(inertia)\n",
    "k_optimal = K_values[np.argmin(diff) + 1]\n",
    "\n",
    "print(f\"The optimal number of clusters (K) is: {k_optimal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)  # Specify the number of components as None to retain all components\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PCA with 2 components\n",
    "pca_2c_model = PCA(n_components=2)\n",
    "x_pca_2c = pca_2c_model.fit_transform(scaled_df)\n",
    "\n",
    "# Explained variance for 2 components\n",
    "explained_variance_2c = pca_2c_model.explained_variance_\n",
    "explained_variance_ratio_2c = pca_2c_model.explained_variance_ratio_\n",
    "\n",
    "print(\"Explained variance by the first 2 components:\")\n",
    "print(explained_variance_2c)\n",
    "print(\"Explained variance ratio by the first 2 components:\")\n",
    "print(explained_variance_ratio_2c)\n",
    "\n",
    "# PCA with 4 components\n",
    "pca_4c_model = PCA(n_components=4)\n",
    "x_pca_4c = pca_4c_model.fit_transform(scaled_df)\n",
    "\n",
    "# Explained variance for 4 components\n",
    "explained_variance_4c = pca_4c_model.explained_variance_\n",
    "explained_variance_ratio_4c = pca_4c_model.explained_variance_ratio_\n",
    "\n",
    "print(\"\\nExplained variance by the first 4 components:\")\n",
    "print(explained_variance_4c)\n",
    "print(\"Explained variance ratio by the first 4 components:\")\n",
    "print(explained_variance_ratio_4c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters in the data \n",
    "optimal_K = 3\n",
    "kmeans = KMeans(n_clusters=optimal_K, random_state=42)\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "# Visualize the clusters using the first two principal components\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(optimal_K):\n",
    "    plt.scatter(X_pca[kmeans.labels_ == i, 0], X_pca[kmeans.labels_ == i, 1], label=f'Cluster {i + 1}')\n",
    "plt.title('Clustering using the first 2 Principal Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>The clusters appear to be well separated, suggesting that the clustering algorithm was able to identify distinct groups in the data.The clear separation of clusters indicates that the PCA and clustering were successful in grouping similar data points together.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into separate training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Cross Validation using XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare parameters\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 8 ,10],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [50, 100, 150 ,200],\n",
    "}\n",
    "\n",
    "# XGBoost classifier\n",
    "xgb_clf = XGBClassifier(objective='binary:logistic', alpha=10, random_state=42)\n",
    "\n",
    "# 5-fold cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# GridSearchCV object to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, scoring='accuracy', cv=cv)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions with XGBoost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the classifier to the training data\n",
    "xgb_clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions with XGBoost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check accuracy score and perform evaluation metrics using classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print accuracy score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Use the best model to make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # Adjust 'average' for multi-class classification\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1-Score:\", f1)\n",
    "\n",
    "# Save the best model to a .h5 file\n",
    "model_filename = 'best_model.h5'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Best model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid_search.predict(X_test) \n",
    "\n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the file path to the saved model\n",
    "h5_file_path = \"best_model.h5\"\n",
    "\n",
    "# Load the model's coefficients and intercept from the .h5 file\n",
    "def load_model():\n",
    "    with h5py.File(h5_file_path, 'r') as h5f:\n",
    "        coef_ = h5f['coef_'][:]\n",
    "        intercept_ = h5f['intercept_'][:]\n",
    "    return coef_, intercept_\n",
    "\n",
    "# Function to predict the channel using loaded model parameters\n",
    "def predict(input_data, coef_, intercept_):\n",
    "    # Calculate the linear combination\n",
    "    linear_combination = np.dot(input_data, coef_.T) + intercept_\n",
    "    # Apply the logistic function to classify as 0 (Horeca) or 1 (Retail)\n",
    "    prediction = (linear_combination > 0).astype(int)\n",
    "    return prediction[0]\n",
    "\n",
    "# Display 10 inferences with true and predicted labels\n",
    "def display_inferences(X_test, y_test, coef_, intercept_):\n",
    "    y_pred = [predict(X_test[i].reshape(1, -1), coef_, intercept_) for i in range(10)]\n",
    "    print(\"\\nSample Predictions (True vs. Predicted):\")\n",
    "    for i in range(10):\n",
    "        true_label = 'Horeca' if y_test[i] == 0 else 'Retail'\n",
    "        predicted_label = 'Horeca' if y_pred[i] == 0 else 'Retail'\n",
    "        print(f\"True: {true_label}, Predicted: {predicted_label}\")\n",
    "\n",
    "# Function for user to input spending values and get a prediction\n",
    "def user_inference(coef_, intercept_):\n",
    "    print(\"\\nEnter the annual spending on the following categories:\")\n",
    "    fresh = float(input(\"FRESH (e.g., 30000): \"))\n",
    "    milk = float(input(\"MILK (e.g., 15000): \"))\n",
    "    grocery = float(input(\"GROCERY (e.g., 20000): \"))\n",
    "    frozen = float(input(\"FROZEN (e.g., 5000): \"))\n",
    "    detergents_paper = float(input(\"DETERGENTS_PAPER (e.g., 3000): \"))\n",
    "    delicatessen = float(input(\"DELICATESSEN (e.g., 2000): \"))\n",
    "\n",
    "    # Prepare input data and predict\n",
    "    user_data = np.array([[fresh, milk, grocery, frozen, detergents_paper, delicatessen]])\n",
    "    prediction = predict(user_data, coef_, intercept_)\n",
    "    print(\"Predicted Channel:\", \"Horeca\" if prediction == 0 else \"Retail\")\n",
    "\n",
    "# Load model parameters\n",
    "coef_, intercept_ = load_model()\n",
    "\n",
    "# Load test data (replace this with actual test data if needed)\n",
    "# Example test data setup\n",
    "test_data = pd.DataFrame({\n",
    "    'Fresh': [30000, 20000, 10000, 5000, 25000, 15000, 30000, 20000, 10000, 5000],\n",
    "    'Milk': [15000, 10000, 5000, 3000, 2000, 7000, 8000, 15000, 10000, 5000],\n",
    "    'Grocery': [20000, 25000, 12000, 1000, 30000, 5000, 7000, 20000, 15000, 2000],\n",
    "    'Frozen': [5000, 3000, 1000, 2000, 4000, 5000, 3000, 2000, 1000, 500],\n",
    "    'Detergents_Paper': [3000, 5000, 2000, 4000, 3000, 1000, 2500, 2000, 3000, 4000],\n",
    "    'Delicassen': [2000, 3000, 1000, 2500, 1500, 1000, 500, 2000, 3000, 1000]\n",
    "})\n",
    "\n",
    "# Generate test labels for demonstration purposes (0: Horeca, 1: Retail)\n",
    "test_labels = np.array([0, 1, 0, 1, 0, 1, 1, 0, 1, 0])\n",
    "\n",
    "# Display sample inferences\n",
    "display_inferences(test_data.values, test_labels, coef_, intercept_)\n",
    "\n",
    "# Run user input inference\n",
    "user_inference(coef_, intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\DELL\\Desktop\\iot-project\\Wholesale customers data.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "print(\"Data is read successfully.\")\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
    "\n",
    "# Feature scaling using MinMaxScaler\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "scaler_MinMax = MinMaxScaler().fit_transform(df[numerical_features])\n",
    "scaled_df = pd.DataFrame(scaler_MinMax, columns=numerical_features)\n",
    "\n",
    "# Elbow method for KMeans Clustering\n",
    "K_values = range(2, 16)\n",
    "inertia = []\n",
    "for k in K_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_df.drop(columns=[\"Channel\"]))\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow method\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K_values, inertia, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(scaled_df.drop(columns=[\"Channel\"]))\n",
    "\n",
    "# Clustering visualization with PCA\n",
    "optimal_K = 3\n",
    "kmeans = KMeans(n_clusters=optimal_K, random_state=42)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(optimal_K):\n",
    "    plt.scatter(X_pca[labels == i, 0], X_pca[labels == i, 1], label=f'Cluster {i + 1}')\n",
    "plt.title('Clustering using the first 2 Principal Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# Define features (X) and target (y)\n",
    "X = scaled_df.drop(columns=['Channel'])\n",
    "\n",
    "# Map the target variable to binary classes (1 -> 0, 2 -> 1)\n",
    "y = df['Channel'].map({1: 0, 2: 1})\n",
    "\n",
    "# Verify the unique values in y\n",
    "print(\"Unique values in target after mapping:\", y.unique())\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Define XGBoost model and GridSearch parameters\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 8, 10],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "}\n",
    "\n",
    "xgb_clf = XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, scoring='accuracy', cv=cv)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Rest of your code for training, evaluation, saving, and user inference...\n",
    "\n",
    "\n",
    "# Best model and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy from cross-validation:\", grid_search.best_score_)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"XGBoost model accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model\n",
    "model_path = r\"C:\\Users\\DELL\\Desktop\\iot-project\\best_model.joblib\"\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Best model saved to {model_path}\")\n",
    "\n",
    "# Load and use the model for inference\n",
    "loaded_model = joblib.load(model_path)\n",
    "\n",
    "# Display sample inferences\n",
    "def display_inferences(X_test, y_test):\n",
    "    y_pred_sample = loaded_model.predict(X_test[:10])\n",
    "    print(\"\\nSample Predictions (True vs. Predicted):\")\n",
    "    for i in range(10):\n",
    "        true_label = 'Horeca' if y_test.iloc[i] == 1 else 'Retail'\n",
    "        predicted_label = 'Horeca' if y_pred_sample[i] == 1 else 'Retail'\n",
    "        print(f\"True: {true_label}, Predicted: {predicted_label}\")\n",
    "\n",
    "display_inferences(X_test, y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem5_py_37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
